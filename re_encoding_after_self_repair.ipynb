{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 1: Data Construction, Visualization & Descriptive Stats (FIXED)\n",
        "# =============================================================================\n",
        "!pip install praat-parselmouth pandas numpy matplotlib seaborn tqdm requests rpy2\n",
        "\n",
        "# üëá „Åì„Çå„Åå R (%%R) „Çí‰Ωø„ÅÜ„Åü„ÇÅ„Å´ÂøÖÈ†à„ÅÆ„Ç≥„Éû„É≥„Éâ„Åß„Åô\n",
        "%load_ext rpy2.ipython\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Setup & Directories (Robust Mount)\n",
        "if not os.path.exists('/content/drive'):\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Initial mount failed. Retrying with force_remount...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/buckeye_self_repair\"\n",
        "RAW_DIR  = os.path.join(BASE_DIR, \"raw\")\n",
        "OUT_DIR  = os.path.join(BASE_DIR, \"output\")\n",
        "for d in [RAW_DIR, OUT_DIR]: os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# 2. Helper Functions\n",
        "def normalize_token(token):\n",
        "    if not token: return \"\"\n",
        "    if token.startswith('<') or token.startswith('{'): return \"\"\n",
        "    return token.strip(string.punctuation).lower()\n",
        "\n",
        "def load_words_robust(filepath):\n",
        "    \"\"\"Buckeye .words loader: Uses next onset for duration calculation.\"\"\"\n",
        "    if not os.path.exists(filepath): return pd.DataFrame()\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 3 and not line.startswith(('#', '{')):\n",
        "                try:\n",
        "                    onset = float(parts[0])\n",
        "                    raw_word = parts[2]\n",
        "                    norm_w = normalize_token(raw_word)\n",
        "                    if norm_w: data.append({\"onset\": onset, \"word\": norm_w})\n",
        "                except ValueError: continue\n",
        "    df = pd.DataFrame(data)\n",
        "    if df.empty: return df\n",
        "    df['offset'] = df['onset'].shift(-1)\n",
        "    df = df.dropna(subset=['offset'])\n",
        "    df['duration'] = df['offset'] - df['onset']\n",
        "    df = df[df['duration'] > 0]\n",
        "    df['nth'] = df.groupby('word').cumcount() + 1\n",
        "    return df\n",
        "\n",
        "def get_acoustic_measures(wav_path, t_min, t_max):\n",
        "    \"\"\"Returns (Intensity, F0_Reliable_Frames, Total_Frames)\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(wav_path): return np.nan, 0, 0\n",
        "        sound = parselmouth.Sound(wav_path)\n",
        "        part = sound.extract_part(from_time=t_min, to_time=t_max, preserve_times=True)\n",
        "\n",
        "        # Intensity\n",
        "        intensity = call(part.to_intensity(), \"Get mean\", t_min, t_max, \"energy\")\n",
        "\n",
        "        # F0 Availability\n",
        "        pitch = part.to_pitch(time_step=0.01, pitch_floor=75, pitch_ceiling=300)\n",
        "        f0_vals = pitch.selected_array['frequency']\n",
        "        n_total = len(f0_vals)\n",
        "        n_reliable = np.sum((f0_vals > 75) & (f0_vals < 300))\n",
        "\n",
        "        return intensity, n_reliable, n_total\n",
        "    except: return np.nan, 0, 0\n",
        "\n",
        "def calculate_local_speech_rate(wdf, target_onset, window=2.0):\n",
        "    start, end = target_onset - window, target_onset + window\n",
        "    count = wdf[(wdf['onset'] >= start) & (wdf['onset'] <= end)].shape[0]\n",
        "    return count / (2 * window)\n",
        "\n",
        "# 3. Load Frequency\n",
        "print(\"üîµ Phase 0: Frequency Data...\")\n",
        "freq_dict = {}\n",
        "try:\n",
        "    r = requests.get(\"https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus/subtlexus2.zip\")\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    with z.open(next(f for f in z.namelist() if 'SUBTLEX' in f and f.endswith('.txt'))) as f:\n",
        "        df_freq = pd.read_csv(f, sep='\\t')\n",
        "        freq_dict = df_freq.set_index(df_freq['Word'].astype(str).str.lower())['Lg10WF'].to_dict()\n",
        "except: print(\"‚ö†Ô∏è Using dummy frequency.\")\n",
        "\n",
        "# 4. Main Processing\n",
        "print(\"üîµ Phase 1: Corpus Scanning & Pairing...\")\n",
        "candidates = []\n",
        "FILLERS = {\"uh\", \"um\", \"mm\", \"hm\", \"ah\", \"eh\"}\n",
        "txt_files = sorted(glob.glob(os.path.join(RAW_DIR, \"*.txt\")))\n",
        "\n",
        "for txt_path in tqdm(txt_files, desc=\"Scanning\"):\n",
        "    base = os.path.splitext(os.path.basename(txt_path))[0]\n",
        "    speaker = base[:3]\n",
        "    with open(txt_path, 'r', encoding='utf-8', errors='ignore') as f: lines = f.readlines()\n",
        "\n",
        "    tokens = []\n",
        "    wc = {}\n",
        "    for line in lines:\n",
        "        l_toks = line.split()\n",
        "        valid_idxs = [i for i, t in enumerate(l_toks) if normalize_token(t)]\n",
        "        for idx, t in enumerate(l_toks):\n",
        "            norm = normalize_token(t)\n",
        "            nth, is_final = 0, 0\n",
        "            if norm:\n",
        "                wc[norm] = wc.get(norm, 0) + 1\n",
        "                nth = wc[norm]\n",
        "                if valid_idxs and idx == valid_idxs[-1]: is_final = 1\n",
        "            tokens.append({\"orig\": t, \"norm\": norm, \"nth\": nth, \"is_final\": is_final})\n",
        "\n",
        "    cutoff_re = re.compile(r\"<CUTOFF-[^>]+>\")\n",
        "    for i, tok in enumerate(tokens):\n",
        "        if cutoff_re.match(tok['orig']) and i > 0:\n",
        "            target = tokens[i-1]['norm']\n",
        "            if not target: continue\n",
        "            for j in range(i+1, min(i+6, len(tokens))):\n",
        "                mid = tokens[i+1:j]\n",
        "                if any(m['orig'].startswith('<HES') or m['norm'] in FILLERS for m in mid): break\n",
        "                if tokens[j]['norm'] == target:\n",
        "                    candidates.append({\n",
        "                        \"basename\": base, \"speaker\": speaker, \"word\": target,\n",
        "                        \"first_nth\": tokens[i-1]['nth'], \"repeat_nth\": tokens[j]['nth'],\n",
        "                        \"first_pos\": tokens[i-1]['is_final'], \"repeat_pos\": tokens[j]['is_final']\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "# Pairing & Acoustics\n",
        "final_pairs = []\n",
        "cand_df = pd.DataFrame(candidates)\n",
        "\n",
        "for base, group in tqdm(cand_df.groupby(\"basename\"), desc=\"Pairing & Acoustics\"):\n",
        "    wdf = load_words_robust(os.path.join(RAW_DIR, f\"{base}.words\"))\n",
        "    if wdf.empty: continue\n",
        "\n",
        "    for _, row in group.iterrows():\n",
        "        f_rows = wdf[(wdf['word']==row['word']) & (wdf['nth']==row['first_nth'])]\n",
        "        r_rows = wdf[(wdf['word']==row['word']) & (wdf['nth']==row['repeat_nth'])]\n",
        "\n",
        "        if not f_rows.empty and not r_rows.empty:\n",
        "            f, r = f_rows.iloc[0], r_rows.iloc[0]\n",
        "\n",
        "            # Acoustics\n",
        "            int_f, f0_k_f, f0_n_f = get_acoustic_measures(os.path.join(RAW_DIR, f\"{base}.wav\"), f['onset'], f['offset'])\n",
        "            int_r, f0_k_r, f0_n_r = get_acoustic_measures(os.path.join(RAW_DIR, f\"{base}.wav\"), r['onset'], r['offset'])\n",
        "\n",
        "            common = {\"basename\": base, \"speaker\": row['speaker'], \"word\": row['word'], \"log_freq\": freq_dict.get(row['word'], 0.0)}\n",
        "\n",
        "            # First\n",
        "            final_pairs.append({**common, \"condition\": \"first\", \"duration\": f['duration'], \"intensity\": int_f,\n",
        "                                \"f0_k\": f0_k_f, \"f0_n\": f0_n_f, \"utt_pos\": row['first_pos'],\n",
        "                                \"speech_rate\": calculate_local_speech_rate(wdf, f['onset'])})\n",
        "            # Repeat\n",
        "            final_pairs.append({**common, \"condition\": \"repeat\", \"duration\": r['duration'], \"intensity\": int_r,\n",
        "                                \"f0_k\": f0_k_r, \"f0_n\": f0_n_r, \"utt_pos\": row['repeat_pos'],\n",
        "                                \"speech_rate\": calculate_local_speech_rate(wdf, r['onset'])})\n",
        "\n",
        "pairs_df = pd.DataFrame(final_pairs)\n",
        "\n",
        "if not pairs_df.empty:\n",
        "    # Cleaning\n",
        "    pairs_df = pairs_df[(pairs_df['duration'] > 0.03) & (pairs_df['duration'] < 2.0)].dropna()\n",
        "    pairs_df['log_duration'] = np.log(pairs_df['duration']) # Seconds basis\n",
        "    pairs_df['intensity_centered'] = pairs_df['intensity'] - pairs_df.groupby('speaker')['intensity'].transform('mean')\n",
        "    pairs_df['f0_avail'] = pairs_df.apply(lambda x: x['f0_k']/x['f0_n'] if x['f0_n']>0 else 0, axis=1)\n",
        "\n",
        "    # Save for R\n",
        "    pairs_df.to_csv(os.path.join(OUT_DIR, \"data_for_r.csv\"), index=False)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # üìä Outputs: Table 1 & Figures\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nüîµ Generating Outputs...\")\n",
        "    sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "    plt.rcParams.update({'text.color':'black', 'axes.labelcolor':'black', 'xtick.color':'black', 'ytick.color':'black'})\n",
        "\n",
        "    # Table 1\n",
        "    table1 = pairs_df.groupby('condition')[['duration', 'log_duration', 'intensity_centered', 'f0_avail']].agg(['mean', 'std', 'count']).T\n",
        "    table1.to_csv(os.path.join(OUT_DIR, \"Table1_Descriptive_Stats.csv\"))\n",
        "    print(f\"‚úÖ Table 1 saved.\")\n",
        "\n",
        "    # Figure 1: Distribution\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.kdeplot(data=pairs_df, x=\"log_duration\", hue=\"condition\", fill=True, palette=['gray', 'black'], alpha=0.4)\n",
        "    plt.title(\"Figure 1: Distribution of Log Duration\", fontweight='bold')\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"Figure1_Distribution.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 2: Comparison\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.pointplot(data=pairs_df, x=\"condition\", y=\"log_duration\", order=[\"first\", \"repeat\"], capsize=.1, color=\"black\", markers=\"s\")\n",
        "    sns.stripplot(data=pairs_df, x=\"condition\", y=\"log_duration\", order=[\"first\", \"repeat\"], color=\"gray\", alpha=0.1, jitter=True)\n",
        "    plt.title(\"Figure 2: Mean Log Duration\", fontweight='bold')\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"Figure2_Comparison.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Figure S1: F0 Availability\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.boxplot(data=pairs_df, x=\"condition\", y=\"f0_avail\", order=[\"first\", \"repeat\"], showfliers=False, color=\"white\", linecolor=\"black\")\n",
        "    sns.stripplot(data=pairs_df, x=\"condition\", y=\"f0_avail\", order=[\"first\", \"repeat\"], color=\"gray\", alpha=0.1, jitter=True)\n",
        "    plt.title(\"Figure S1: F0 Availability\", fontweight='bold')\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"FigureS1_F0_Availability.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"‚úÖ All Figures (1, 2, S1) saved to {OUT_DIR}\")\n",
        "    print(\"üëâ Proceed to Cell 2 for Statistical Modeling.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No valid pairs found.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praat-parselmouth\n",
            "  Downloading praat_parselmouth-0.4.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.12/dist-packages (3.5.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: cffi>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from rpy2) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from rpy2) (3.1.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from rpy2) (5.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.15.1->rpy2) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->rpy2) (3.0.3)\n",
            "Downloading praat_parselmouth-0.4.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: praat-parselmouth\n",
            "Successfully installed praat-parselmouth-0.4.7\n",
            "Mounted at /content/drive\n",
            "üîµ Phase 0: Frequency Data...\n",
            "üîµ Phase 1: Corpus Scanning & Pairing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255/255 [00:56<00:00,  4.50it/s]\n",
            "Pairing & Acoustics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [02:29<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîµ Generating Outputs...\n",
            "‚úÖ Table 1 saved.\n",
            "‚úÖ All Figures (1, 2, S1) saved to /content/drive/MyDrive/buckeye_self_repair/output\n",
            "üëâ Proceed to Cell 2 for Statistical Modeling.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiP2ltTrEKaY",
        "outputId": "d86a5d59-2488-47db-8d4a-b6916e0bb5f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# =============================================================================\n",
        "# Cell 2: R Statistical Modeling (Saves Results to Files)\n",
        "# =============================================================================\n",
        "\n",
        "# Install/Load\n",
        "if (!require(\"lme4\")) install.packages(\"lme4\", quiet=TRUE)\n",
        "if (!require(\"lmerTest\")) install.packages(\"lmerTest\", quiet=TRUE)\n",
        "if (!require(\"MuMIn\")) install.packages(\"MuMIn\", quiet=TRUE)\n",
        "library(lme4); library(lmerTest); library(MuMIn)\n",
        "\n",
        "# Load Data\n",
        "out_dir <- \"/content/drive/MyDrive/buckeye_self_repair/output\"\n",
        "data <- read.csv(file.path(out_dir, \"data_for_r.csv\"))\n",
        "\n",
        "# Factorize\n",
        "data$condition <- as.factor(data$condition)\n",
        "data$speaker <- as.factor(data$speaker)\n",
        "data$word <- as.factor(data$word)\n",
        "data$utt_pos <- as.factor(data$utt_pos)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Duration Model (Table 2)\n",
        "# ---------------------------------------------------------\n",
        "sink(file.path(out_dir, \"Table2_Duration_Model.txt\"))\n",
        "cat(\"=== TABLE 2: DURATION MODEL ===\\n\")\n",
        "m_dur <- lmer(log_duration ~ condition + log_freq + speech_rate + utt_pos + (1|speaker) + (1|word), data=data, REML=TRUE)\n",
        "print(summary(m_dur))\n",
        "print(r.squaredGLMM(m_dur))\n",
        "sink()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Intensity Model (Table 3)\n",
        "# ---------------------------------------------------------\n",
        "sink(file.path(out_dir, \"Table3_Intensity_Model.txt\"))\n",
        "cat(\"=== TABLE 3: INTENSITY MODEL ===\\n\")\n",
        "m_int <- lmer(intensity_centered ~ condition + log_freq + speech_rate + utt_pos + (1|speaker) + (1|word), data=data, REML=TRUE)\n",
        "print(summary(m_int))\n",
        "print(r.squaredGLMM(m_int))\n",
        "sink()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. F0 Availability Model (Supplementary)\n",
        "# ---------------------------------------------------------\n",
        "data$f0_fail <- data$f0_n - data$f0_k\n",
        "sink(file.path(out_dir, \"Supplementary_F0_Model.txt\"))\n",
        "cat(\"=== SUPPLEMENTARY: F0 MODEL ===\\n\")\n",
        "m_f0 <- glmer(cbind(f0_k, f0_fail) ~ condition + log_freq + speech_rate + utt_pos + (1|speaker), data=data, family=binomial, control=glmerControl(optimizer=\"bobyqa\"))\n",
        "print(summary(m_f0))\n",
        "sink()\n",
        "\n",
        "cat(\"‚úÖ All Statistical Tables saved to:\", out_dir, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "l5AP_RfbEXsI",
        "outputId": "52a5dc42-733a-40df-9edc-02af79a5c1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All Statistical Tables saved to: /content/drive/MyDrive/buckeye_self_repair/output \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading required package: lme4\n",
              "also installing the dependencies ‚Äòrbibutils‚Äô, ‚Äòminqa‚Äô, ‚Äònloptr‚Äô, ‚Äòreformulas‚Äô, ‚ÄòRdpack‚Äô, ‚ÄòRcppEigen‚Äô\n",
              "\n",
              "Loading required package: lmerTest\n",
              "also installing the dependency ‚ÄònumDeriv‚Äô\n",
              "\n",
              "Loading required package: MuMIn\n",
              "also installing the dependency ‚Äòinsight‚Äô\n",
              "\n",
              "Loading required package: Matrix\n",
              "\n",
              "Attaching package: ‚ÄòlmerTest‚Äô\n",
              "\n",
              "The following object is masked from ‚Äòpackage:lme4‚Äô:\n",
              "\n",
              "    lmer\n",
              "\n",
              "The following object is masked from ‚Äòpackage:stats‚Äô:\n",
              "\n",
              "    step\n",
              "\n",
              "boundary (singular) fit: see help('isSingular')\n",
              "In addition: Warning messages:\n",
              "1: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :\n",
              "  there is no package called ‚Äòlme4‚Äô\n",
              "2: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :\n",
              "  there is no package called ‚ÄòlmerTest‚Äô\n",
              "3: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :\n",
              "  there is no package called ‚ÄòMuMIn‚Äô\n",
              "4: Model failed to converge with 1 negative eigenvalue: -3.5e+02 \n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# =============================================================================\n",
        "# Cell 5: Interaction Analysis via Likelihood Ratio Tests (LRT)\n",
        "# =============================================================================\n",
        "\n",
        "# Libraries\n",
        "library(lme4)\n",
        "library(lmerTest)\n",
        "\n",
        "# Load Data\n",
        "out_dir <- \"/content/drive/MyDrive/buckeye_self_repair/output\"\n",
        "data <- read.csv(file.path(out_dir, \"data_for_r.csv\"))\n",
        "\n",
        "# Factorize\n",
        "data$condition <- as.factor(data$condition)\n",
        "data$speaker <- as.factor(data$speaker)\n",
        "data$word <- as.factor(data$word)\n",
        "data$utt_pos <- as.factor(data$utt_pos)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. DURATION MODEL INTERACTIONS (log_duration)\n",
        "# -----------------------------------------------------------------------------\n",
        "cat(\"\\nüîµ --- LRT: DURATION INTERACTIONS ---\\n\")\n",
        "\n",
        "# Base Model (ML for comparison)\n",
        "m0_dur <- lmer(log_duration ~ condition + log_freq + speech_rate + utt_pos + (1|speaker) + (1|word),\n",
        "               data=data, REML=FALSE)\n",
        "\n",
        "# Interaction 1: Condition x Speech Rate\n",
        "m1_dur <- lmer(log_duration ~ condition * speech_rate + log_freq + utt_pos + (1|speaker) + (1|word),\n",
        "               data=data, REML=FALSE)\n",
        "\n",
        "# Interaction 2: Condition x Utterance Position\n",
        "m2_dur <- lmer(log_duration ~ condition * utt_pos + log_freq + speech_rate + (1|speaker) + (1|word),\n",
        "               data=data, REML=FALSE)\n",
        "\n",
        "cat(\"\\n>> Test 1: Condition x Speech Rate (Duration)\\n\")\n",
        "print(anova(m0_dur, m1_dur))\n",
        "\n",
        "cat(\"\\n>> Test 2: Condition x Utt Position (Duration)\\n\")\n",
        "print(anova(m0_dur, m2_dur))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. INTENSITY MODEL INTERACTIONS (intensity_centered)\n",
        "# -----------------------------------------------------------------------------\n",
        "cat(\"\\n\\nüîµ --- LRT: INTENSITY INTERACTIONS ---\\n\")\n",
        "\n",
        "# Base Model (ML)\n",
        "m0_int <- lmer(intensity_centered ~ condition + log_freq + speech_rate + utt_pos + (1|speaker) + (1|word),\n",
        "               data=data, REML=FALSE)\n",
        "\n",
        "# Interaction 1: Condition x Speech Rate\n",
        "m1_int <- lmer(intensity_centered ~ condition * speech_rate + log_freq + utt_pos + (1|speaker) + (1|word),\n",
        "               data=data, REML=FALSE)\n",
        "\n",
        "# Interaction 2: Condition x Utterance Position\n",
        "m2_int <- lmer(intensity_centered ~ condition * utt_pos + log_freq + speech_rate + (1|speaker) + (1|word),\n",
        "               data=data, REML=FALSE)\n",
        "\n",
        "cat(\"\\n>> Test 3: Condition x Speech Rate (Intensity)\\n\")\n",
        "print(anova(m0_int, m1_int))\n",
        "\n",
        "cat(\"\\n>> Test 4: Condition x Utt Position (Intensity)\\n\")\n",
        "print(anova(m0_int, m2_int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "2hXgqCGFCxAG",
        "outputId": "fea4ed37-b316-4f25-a75a-fcbd16973dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîµ --- LRT: DURATION INTERACTIONS ---\n",
            "\n",
            ">> Test 1: Condition x Speech Rate (Duration)\n",
            "Data: data\n",
            "Models:\n",
            "m0_dur: log_duration ~ condition + log_freq + speech_rate + utt_pos + (1 | speaker) + (1 | word)\n",
            "m1_dur: log_duration ~ condition * speech_rate + log_freq + utt_pos + (1 | speaker) + (1 | word)\n",
            "       npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(>Chisq)\n",
            "m0_dur    8 1438.7 1478.7 -711.37    1422.7                     \n",
            "m1_dur    9 1440.7 1485.7 -711.36    1422.7 0.0064  1     0.9365\n",
            "\n",
            ">> Test 2: Condition x Utt Position (Duration)\n",
            "Data: data\n",
            "Models:\n",
            "m0_dur: log_duration ~ condition + log_freq + speech_rate + utt_pos + (1 | speaker) + (1 | word)\n",
            "m2_dur: log_duration ~ condition * utt_pos + log_freq + speech_rate + (1 | speaker) + (1 | word)\n",
            "       npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(>Chisq)\n",
            "m0_dur    8 1438.7 1478.7 -711.37    1422.7                     \n",
            "m2_dur    9 1440.4 1485.3 -711.18    1422.4 0.3764  1     0.5396\n",
            "\n",
            "\n",
            "üîµ --- LRT: INTENSITY INTERACTIONS ---\n",
            "\n",
            ">> Test 3: Condition x Speech Rate (Intensity)\n",
            "Data: data\n",
            "Models:\n",
            "m0_int: intensity_centered ~ condition + log_freq + speech_rate + utt_pos + (1 | speaker) + (1 | word)\n",
            "m1_int: intensity_centered ~ condition * speech_rate + log_freq + utt_pos + (1 | speaker) + (1 | word)\n",
            "       npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(>Chisq)\n",
            "m0_int    8 9006.1 9046.1 -4495.1    8990.1                     \n",
            "m1_int    9 9008.0 9053.0 -4495.0    8990.0 0.0965  1     0.7561\n",
            "\n",
            ">> Test 4: Condition x Utt Position (Intensity)\n",
            "Data: data\n",
            "Models:\n",
            "m0_int: intensity_centered ~ condition + log_freq + speech_rate + utt_pos + (1 | speaker) + (1 | word)\n",
            "m2_int: intensity_centered ~ condition * utt_pos + log_freq + speech_rate + (1 | speaker) + (1 | word)\n",
            "       npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(>Chisq)\n",
            "m0_int    8 9006.1 9046.1 -4495.1    8990.1                     \n",
            "m2_int    9 9008.1 9053.0 -4495.1    8990.1 0.0313  1     0.8596\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "boundary (singular) fit: see help('isSingular')\n",
              "boundary (singular) fit: see help('isSingular')\n",
              "boundary (singular) fit: see help('isSingular')\n",
              "In addition: Warning messages:\n",
              "1: Model failed to converge with 1 negative eigenvalue: -3.3e+02 \n",
              "2: Model failed to converge with 1 negative eigenvalue: -3.4e+02 \n",
              "3: Model failed to converge with 1 negative eigenvalue: -3.3e+02 \n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Supplementary Analysis: F0 < 50% Calculation (Python)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "\n",
        "# „Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÔºà„ÇÇ„Åó„É°„É¢„É™„Å´„Å™„Åë„Çå„Å∞Ôºâ\n",
        "if 'pairs_df' not in locals():\n",
        "    pairs_df = pd.read_csv(\"/content/drive/MyDrive/buckeye_self_repair/output/data_for_r.csv\")\n",
        "\n",
        "# F0 Availability „Åå 0.5 Êú™Ê∫Ä„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÂà§ÂÆö\n",
        "# (f0_availÂàó„ÅØ„Åô„Åß„Å´Ë®àÁÆóÊ∏à„Åø: f0_k / f0_n)\n",
        "pairs_df['lt50'] = pairs_df['f0_avail'] < 0.5\n",
        "\n",
        "# ÈõÜË®à\n",
        "f0_stats = pairs_df.groupby('condition')['lt50'].agg(['sum', 'mean', 'count'])\n",
        "f0_stats['percent'] = f0_stats['mean'] * 100\n",
        "\n",
        "print(\"üìä Tokens with F0 Availability < 50%:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"First Production:       N = {int(f0_stats.loc['first', 'sum'])} ({f0_stats.loc['first', 'percent']:.1f}%)\")\n",
        "print(f\"Post-Repair Repetition: N = {int(f0_stats.loc['repeat', 'sum'])} ({f0_stats.loc['repeat', 'percent']:.1f}%)\")\n",
        "print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avj7I7VGgajd",
        "outputId": "65ef6a4b-cc3d-423d-8a76-44af5e5a8b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Tokens with F0 Availability < 50%:\n",
            "----------------------------------------\n",
            "First Production:       N = 265 (48.4%)\n",
            "Post-Repair Repetition: N = 113 (20.9%)\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}